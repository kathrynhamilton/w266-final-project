{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunk the large json into smaller files, each with 100,000 max rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, islice\n",
    "\n",
    "def chunks(iterable, n):\n",
    "    iterable = iter(iterable)\n",
    "    while True:\n",
    "        yield chain([next(iterable)], islice(iterable, n-1))\n",
    "\n",
    "#10000\n",
    "l = 10*10**4\n",
    "\n",
    "file_large = 'layer1.json'\n",
    "with open(file_large, encoding='utf-8') as bigfile:\n",
    "    for i, lines in enumerate(chunks(bigfile, l)):\n",
    "        file_split = '{}.{}'.format(file_large, i)\n",
    "        with open(file_split, 'w', encoding='utf-8') as f:\n",
    "            f.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use sed and jq to fix json formatting... \n",
    "* first file needs a closing \"]\"\n",
    "* middle files need to delete the \",\" and add both opening \"[\" and closing \"]\"\n",
    "* last file needs to add opening \"[\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i '$ s/,$/]/' layer1.json.0 && cat layer1.json.0 | jq -c --slurp . > layer1.json.0.valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(1, 10):\n",
    "    !sed -i '1s/^/[\\n/' layer1.json.{idx} && sed -i '$ s/,$/]/' layer1.json.{idx} && cat layer1.json.{idx} | jq -c --slurp . > layer1.json.{idx}.valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i '1s/^/[\\n/' layer1.json.10 && cat layer1.json.10 | jq -c --slurp . > layer1.json.10.valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load json files into compressed pickle files for reusability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "files = [\"layer1.json.0.valid\",\n",
    "         \"layer1.json.1.valid\",\n",
    "         \"layer1.json.2.valid\",\n",
    "         \"layer1.json.3.valid\",\n",
    "         \"layer1.json.4.valid\",\n",
    "         \"layer1.json.5.valid\",\n",
    "         \"layer1.json.6.valid\",\n",
    "         \"layer1.json.7.valid\",\n",
    "         \"layer1.json.8.valid\",\n",
    "         \"layer1.json.9.valid\"]\n",
    "\n",
    "for each in files:\n",
    "    tmp_df = pd.read_json(each, lines=True)\n",
    "    new_df = pd.concat([pd.DataFrame(json_normalize(x)) for x in tmp_df[0]],\n",
    "                       ignore_index=True)\n",
    "    \n",
    "    # add ingredient and instruction counts and shorten url\n",
    "    new_df[\"ingr_count\"] = new_df.apply(lambda row: len(row.ingredients), axis=1)\n",
    "    new_df[\"instr_count\"] = new_df.apply(lambda row: len(row.instructions), axis=1)\n",
    "    new_df[\"collection\"] = new_df.apply(lambda row: row.url.split(\"//\")[-1].split(\"/\")[0].split('?')[0], axis=1)\n",
    "    new_df.to_pickle(\"./\" + each + \".gz\", compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
